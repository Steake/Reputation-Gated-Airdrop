{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EBSL + EZKL Pipeline (Complete Implementation)\n",
        "\n",
        "**Fixed: overflow-safe packing/rebasing + stable product + Jupyter compatibility**\n",
        "\n",
        "This notebook implements a comprehensive Evidence-Based Subjective Logic (EBSL) pipeline with EZKL zero-knowledge proof generation:\n",
        "\n",
        "- Single-input ONNX: combined_input = concat(flat(opinions), flat(mask))\n",
        "- Robust EZKL settings: decomp_legs↑, safe rebasing knobs, version-safe fallbacks\n",
        "- Stable product via log/exp, sign-preserving denominator clamp\n",
        "- Async-safe ezkl calls, CLI SRS fallback, verbose timing & run report\n",
        "- **Jupyter-compatible asyncio handling**\n",
        "\n",
        "## Key Features\n",
        "- **Enhanced EBSL Algorithm**: ZK-optimized fusion with overflow protection\n",
        "- **Property-based Testing**: Hypothesis-driven correctness validation\n",
        "- **Performance Analysis**: Comparative benchmarking\n",
        "- **Robust EZKL Integration**: Multiple fallback strategies\n",
        "- **Jupyter-Compatible**: Fixed asyncio handling for notebook environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision\n",
        "%pip install ezkl\n",
        "%pip install hypothesis\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install networkx\n",
        "%pip install plotly\n",
        "%pip install onnx\n",
        "%pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure matplotlib for inline plotting in Jupyter\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "import traceback\n",
        "from dataclasses import dataclass, asdict\n",
        "from contextlib import contextmanager\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
        "from matplotlib.collections import LineCollection\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from hypothesis import given, strategies as st, settings as hyp_settings\n",
        "\n",
        "import asyncio\n",
        "import inspect\n",
        "import subprocess\n",
        "from pathlib import Path as _Path\n",
        "\n",
        "import ezkl\n",
        "import onnx\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure matplotlib for inline plots\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Logging and Utility Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class StepResult:\n",
        "    name: str\n",
        "    ok: bool\n",
        "    seconds: float\n",
        "    extra: dict\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, verbose: bool = True):\n",
        "        self.verbose = verbose\n",
        "        self.steps = []\n",
        "\n",
        "    def banner(self, title: str):\n",
        "        line = \"=\" * 78\n",
        "        print(f\"\\n{line}\\n{title}\\n{line}\")\n",
        "\n",
        "    def info(self, msg: str):\n",
        "        if self.verbose:\n",
        "            print(msg)\n",
        "\n",
        "    def warn(self, msg: str):\n",
        "        print(f\"[!] {msg}\")\n",
        "\n",
        "    def error(self, msg: str):\n",
        "        print(f\"[✗] {msg}\")\n",
        "\n",
        "    def ok(self, msg: str):\n",
        "        print(f\"[✓] {msg}\")\n",
        "\n",
        "    @contextmanager\n",
        "    def timed(self, name: str, extra: Optional[Dict[str, Any]] = None):\n",
        "        start = time.perf_counter()\n",
        "        ok = True\n",
        "        info = dict(extra or {})\n",
        "        try:\n",
        "            yield info\n",
        "            ok = True\n",
        "        except Exception as e:\n",
        "            ok = False\n",
        "            info[\"exception\"] = repr(e)\n",
        "            info[\"traceback\"] = traceback.format_exc(limit=12)\n",
        "            self.error(f\"{name} failed: {e}\")\n",
        "            raise\n",
        "        finally:\n",
        "            dur = time.perf_counter() - start\n",
        "            self.steps.append(StepResult(name, ok, dur, info))\n",
        "            status = \"OK\" if ok else \"FAIL\"\n",
        "            self.info(f\"[{status}] {name} in {dur:.3f}s\")\n",
        "\n",
        "    def dump_report(self, path: str):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump([asdict(s) for s in self.steps], f, indent=2)\n",
        "        self.ok(f\"Run report written: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EBSLVisualizer:\n",
        "    \"\"\"\n",
        "    Comprehensive visualization toolkit for EBSL pipeline components\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_network_topology(num_nodes=10, trust_matrix=None, title=\"Trust Network Topology\"):\n",
        "        \"\"\"\n",
        "        Create a network topology visualization showing trust relationships\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "        \n",
        "        # Generate or use provided trust matrix\n",
        "        if trust_matrix is None:\n",
        "            # Create a realistic trust matrix\n",
        "            np.random.seed(42)\n",
        "            trust_matrix = np.random.beta(2, 5, (num_nodes, num_nodes))\n",
        "            np.fill_diagonal(trust_matrix, 1.0)  # Self-trust = 1\n",
        "        \n",
        "        # Create network graph\n",
        "        G = nx.DiGraph()\n",
        "        positions = nx.circular_layout(range(num_nodes))\n",
        "        \n",
        "        # Add nodes with reputation-based sizing\n",
        "        for i in range(num_nodes):\n",
        "            reputation = np.mean(trust_matrix[:, i])  # Average incoming trust\n",
        "            G.add_node(i, reputation=reputation)\n",
        "        \n",
        "        # Add edges with trust-based coloring\n",
        "        threshold = 0.3  # Minimum trust to show edge\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                if i != j and trust_matrix[i, j] > threshold:\n",
        "                    G.add_edge(i, j, weight=trust_matrix[i, j])\n",
        "        \n",
        "        # Plot network graph\n",
        "        node_sizes = [G.nodes[i]['reputation'] * 1000 for i in G.nodes()]\n",
        "        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "        \n",
        "        nx.draw_networkx_nodes(G, positions, node_size=node_sizes, \n",
        "                              node_color=[G.nodes[i]['reputation'] for i in G.nodes()],\n",
        "                              cmap='viridis', alpha=0.8, ax=ax1)\n",
        "        nx.draw_networkx_edges(G, positions, width=[w*3 for w in edge_weights],\n",
        "                              edge_color=edge_weights, edge_cmap=plt.cm.plasma,\n",
        "                              alpha=0.6, arrows=True, arrowsize=20, ax=ax1)\n",
        "        nx.draw_networkx_labels(G, positions, ax=ax1)\n",
        "        \n",
        "        ax1.set_title(f\"{title}\\n(Node size ∝ reputation, Edge color/width ∝ trust)\")\n",
        "        ax1.axis('off')\n",
        "        \n",
        "        # Plot trust matrix heatmap\n",
        "        im = ax2.imshow(trust_matrix, cmap='RdYlBu_r', aspect='auto')\n",
        "        ax2.set_title(\"Trust Matrix Heatmap\")\n",
        "        ax2.set_xlabel(\"Trusted Agent\")\n",
        "        ax2.set_ylabel(\"Trusting Agent\")\n",
        "        \n",
        "        # Add colorbar\n",
        "        plt.colorbar(im, ax=ax2, label='Trust Level')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return trust_matrix\n",
        "    \n",
        "    @staticmethod\n",
        "    def visualize_opinion_components(opinions_tensor, title=\"EBSL Opinion Components\"):\n",
        "        \"\"\"\n",
        "        Visualize belief, disbelief, uncertainty, and base rate components\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        \n",
        "        # Extract components\n",
        "        beliefs = opinions_tensor[:, 0].detach().numpy()\n",
        "        disbeliefs = opinions_tensor[:, 1].detach().numpy()\n",
        "        uncertainties = opinions_tensor[:, 2].detach().numpy()\n",
        "        base_rates = opinions_tensor[:, 3].detach().numpy()\n",
        "        \n",
        "        # Plot distributions\n",
        "        axes[0, 0].hist(beliefs, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "        axes[0, 0].set_title('Belief Distribution')\n",
        "        axes[0, 0].set_xlabel('Belief Value')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].axvline(np.mean(beliefs), color='red', linestyle='--', label=f'Mean: {np.mean(beliefs):.3f}')\n",
        "        axes[0, 0].legend()\n",
        "        \n",
        "        axes[0, 1].hist(disbeliefs, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
        "        axes[0, 1].set_title('Disbelief Distribution')\n",
        "        axes[0, 1].set_xlabel('Disbelief Value')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "        axes[0, 1].axvline(np.mean(disbeliefs), color='blue', linestyle='--', label=f'Mean: {np.mean(disbeliefs):.3f}')\n",
        "        axes[0, 1].legend()\n",
        "        \n",
        "        axes[1, 0].hist(uncertainties, bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
        "        axes[1, 0].set_title('Uncertainty Distribution')\n",
        "        axes[1, 0].set_xlabel('Uncertainty Value')\n",
        "        axes[1, 0].set_ylabel('Frequency')\n",
        "        axes[1, 0].axvline(np.mean(uncertainties), color='purple', linestyle='--', label=f'Mean: {np.mean(uncertainties):.3f}')\n",
        "        axes[1, 0].legend()\n",
        "        \n",
        "        axes[1, 1].hist(base_rates, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
        "        axes[1, 1].set_title('Base Rate Distribution')\n",
        "        axes[1, 1].set_xlabel('Base Rate Value')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "        axes[1, 1].axvline(np.mean(base_rates), color='green', linestyle='--', label=f'Mean: {np.mean(base_rates):.3f}')\n",
        "        axes[1, 1].legend()\n",
        "        \n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Create a combined ternary-like plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        \n",
        "        # Scatter plot of belief vs disbelief, colored by uncertainty\n",
        "        scatter = ax.scatter(beliefs, disbeliefs, c=uncertainties, s=100, \n",
        "                           cmap='viridis', alpha=0.7, edgecolors='black')\n",
        "        ax.set_xlabel('Belief')\n",
        "        ax.set_ylabel('Disbelief')\n",
        "        ax.set_title('Opinion Space (Belief vs Disbelief, colored by Uncertainty)')\n",
        "        \n",
        "        # Add constraint line (b + d + u = 1)\n",
        "        x = np.linspace(0, 1, 100)\n",
        "        for u_val in np.linspace(0, 1, 6):\n",
        "            y = 1 - u_val - x\n",
        "            mask = (y >= 0) & (y <= 1)\n",
        "            ax.plot(x[mask], y[mask], '--', alpha=0.3, label=f'u={u_val:.1f}' if u_val in [0, 0.5, 1] else None)\n",
        "        \n",
        "        plt.colorbar(scatter, label='Uncertainty')\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def visualize_zk_circuit_concept(title=\"ZK Circuit Structure Concept\"):\n",
        "        \"\"\"\n",
        "        Visualize the conceptual structure of the ZK circuit\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(14, 10))\n",
        "        \n",
        "        # Define circuit components\n",
        "        components = {\n",
        "            'input': {'pos': (1, 8), 'size': (2, 1), 'color': 'lightblue', 'label': 'Private Input\\n(Opinions + Mask)'},\n",
        "            'unpack': {'pos': (4, 8), 'size': (2, 1), 'color': 'lightgreen', 'label': 'Unpack & Validate\\nOpinions'},\n",
        "            'weight': {'pos': (7, 9), 'size': (2, 0.8), 'color': 'yellow', 'label': 'Weight\\nCalculation'},\n",
        "            'fusion': {'pos': (7, 7), 'size': (2, 0.8), 'color': 'orange', 'label': 'EBSL Fusion\\nAlgorithm'},\n",
        "            'reputation': {'pos': (10, 8), 'size': (2, 1), 'color': 'pink', 'label': 'Reputation\\nCalculation'},\n",
        "            'output': {'pos': (13, 8), 'size': (2, 1), 'color': 'lightcoral', 'label': 'Public Output\\n(Reputation Score)'}\n",
        "        }\n",
        "        \n",
        "        # Draw components\n",
        "        for name, comp in components.items():\n",
        "            rect = FancyBboxPatch(\n",
        "                comp['pos'], comp['size'][0], comp['size'][1],\n",
        "                boxstyle=\"round,pad=0.1\",\n",
        "                facecolor=comp['color'],\n",
        "                edgecolor='black',\n",
        "                linewidth=2\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "            \n",
        "            # Add label\n",
        "            ax.text(comp['pos'][0] + comp['size'][0]/2, comp['pos'][1] + comp['size'][1]/2,\n",
        "                   comp['label'], ha='center', va='center', fontsize=9, weight='bold')\n",
        "        \n",
        "        # Draw connections\n",
        "        connections = [\n",
        "            ('input', 'unpack'),\n",
        "            ('unpack', 'weight'),\n",
        "            ('unpack', 'fusion'),\n",
        "            ('weight', 'reputation'),\n",
        "            ('fusion', 'reputation'),\n",
        "            ('reputation', 'output')\n",
        "        ]\n",
        "        \n",
        "        for start, end in connections:\n",
        "            start_pos = components[start]['pos']\n",
        "            end_pos = components[end]['pos']\n",
        "            start_x = start_pos[0] + components[start]['size'][0]\n",
        "            start_y = start_pos[1] + components[start]['size'][1]/2\n",
        "            end_x = end_pos[0]\n",
        "            end_y = end_pos[1] + components[end]['size'][1]/2\n",
        "            \n",
        "            ax.arrow(start_x, start_y, end_x - start_x - 0.1, end_y - start_y,\n",
        "                    head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
        "        \n",
        "        # Add ZK properties annotations\n",
        "        ax.text(8, 5, '🔒 Zero-Knowledge Properties:', fontsize=12, weight='bold')\n",
        "        ax.text(8, 4.5, '• Input opinions remain private', fontsize=10)\n",
        "        ax.text(8, 4.1, '• Computation is verifiable', fontsize=10)\n",
        "        ax.text(8, 3.7, '• No trust in prover required', fontsize=10)\n",
        "        ax.text(8, 3.3, '• Succinctly verifiable proof', fontsize=10)\n",
        "        \n",
        "        # Add circuit complexity info\n",
        "        ax.text(8, 2.5, '⚡ Circuit Complexity:', fontsize=12, weight='bold')\n",
        "        ax.text(8, 2.1, '• Arithmetic operations only', fontsize=10)\n",
        "        ax.text(8, 1.7, '• Fixed-size constraint system', fontsize=10)\n",
        "        ax.text(8, 1.3, '• Overflow-safe operations', fontsize=10)\n",
        "        \n",
        "        ax.set_xlim(0, 16)\n",
        "        ax.set_ylim(0, 10)\n",
        "        ax.set_title(title, fontsize=16, weight='bold')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_fusion_process(opinions_before, opinion_after, title=\"EBSL Fusion Process\"):\n",
        "        \"\"\"\n",
        "        Visualize the opinion fusion process\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        \n",
        "        # Before fusion - individual opinions\n",
        "        components = ['Belief', 'Disbelief', 'Uncertainty', 'Base Rate']\n",
        "        x = np.arange(len(components))\n",
        "        width = 0.8 / len(opinions_before)\n",
        "        \n",
        "        for i, opinion in enumerate(opinions_before):\n",
        "            offset = (i - len(opinions_before)/2 + 0.5) * width\n",
        "            ax1.bar(x + offset, opinion.detach().numpy(), width, \n",
        "                   label=f'Agent {i+1}', alpha=0.7)\n",
        "        \n",
        "        ax1.set_xlabel('Opinion Components')\n",
        "        ax1.set_ylabel('Value')\n",
        "        ax1.set_title('Before Fusion: Individual Opinions')\n",
        "        ax1.set_xticks(x)\n",
        "        ax1.set_xticklabels(components)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # After fusion - fused opinion\n",
        "        ax2.bar(components, opinion_after.detach().numpy(), \n",
        "               color=['green', 'red', 'orange', 'blue'], alpha=0.7)\n",
        "        ax2.set_xlabel('Opinion Components')\n",
        "        ax2.set_ylabel('Value')\n",
        "        ax2.set_title('After Fusion: Aggregated Opinion')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add values on bars\n",
        "        for i, v in enumerate(opinion_after.detach().numpy()):\n",
        "            ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "        \n",
        "        # Comparison radar chart\n",
        "        angles = np.linspace(0, 2 * np.pi, len(components), endpoint=False)\n",
        "        angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
        "        \n",
        "        # Plot individual opinions\n",
        "        for i, opinion in enumerate(opinions_before[:3]):  # Show only first 3 for clarity\n",
        "            values = opinion.detach().numpy().tolist()\n",
        "            values += [values[0]]  # Complete the circle\n",
        "            ax3.plot(angles, values, 'o-', linewidth=2, label=f'Agent {i+1}', alpha=0.7)\n",
        "        \n",
        "        # Plot fused opinion\n",
        "        fused_values = opinion_after.detach().numpy().tolist()\n",
        "        fused_values += [fused_values[0]]\n",
        "        ax3.plot(angles, fused_values, 'o-', linewidth=3, label='Fused', color='black')\n",
        "        ax3.fill(angles, fused_values, alpha=0.25, color='black')\n",
        "        \n",
        "        ax3.set_xticks(angles[:-1])\n",
        "        ax3.set_xticklabels(components)\n",
        "        ax3.set_ylim(0, 1)\n",
        "        ax3.set_title('Radar Comparison')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "        \n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_performance_comparison(classical_times, zk_times, opinion_counts, title=\"Performance Comparison\"):\n",
        "        \"\"\"\n",
        "        Plot performance comparison between classical and ZK-friendly algorithms\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Execution time comparison\n",
        "        ax1.plot(opinion_counts, classical_times, 'o-', label='Classical EBSL', linewidth=2, markersize=8)\n",
        "        ax1.plot(opinion_counts, zk_times, 's-', label='ZK-Friendly EBSL', linewidth=2, markersize=8)\n",
        "        ax1.set_xlabel('Number of Opinions')\n",
        "        ax1.set_ylabel('Execution Time (seconds)')\n",
        "        ax1.set_title('Execution Time vs Number of Opinions')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.set_yscale('log')\n",
        "        \n",
        "        # Relative performance (speedup/slowdown)\n",
        "        relative_perf = np.array(classical_times) / np.array(zk_times)\n",
        "        ax2.plot(opinion_counts, relative_perf, 'o-', color='purple', linewidth=2, markersize=8)\n",
        "        ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Equal Performance')\n",
        "        ax2.set_xlabel('Number of Opinions')\n",
        "        ax2.set_ylabel('Relative Performance (Classical/ZK-Friendly)')\n",
        "        ax2.set_title('Relative Performance Ratio')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add annotations for significant points\n",
        "        max_speedup_idx = np.argmax(relative_perf)\n",
        "        ax2.annotate(f'Max ratio: {relative_perf[max_speedup_idx]:.2f}x\\nat {opinion_counts[max_speedup_idx]} opinions',\n",
        "                    xy=(opinion_counts[max_speedup_idx], relative_perf[max_speedup_idx]),\n",
        "                    xytext=(10, 10), textcoords='offset points',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
        "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "        \n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_reputation_evolution(reputation_scores, agents, title=\"Reputation Score Evolution\"):\n",
        "        \"\"\"\n",
        "        Plot how reputation scores evolve or are distributed\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Reputation distribution\n",
        "        ax1.hist(reputation_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax1.axvline(np.mean(reputation_scores), color='red', linestyle='--', \n",
        "                   label=f'Mean: {np.mean(reputation_scores):.3f}')\n",
        "        ax1.axvline(np.median(reputation_scores), color='green', linestyle='--', \n",
        "                   label=f'Median: {np.median(reputation_scores):.3f}')\n",
        "        ax1.set_xlabel('Reputation Score')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title('Reputation Score Distribution')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Individual agent reputation\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(agents)))\n",
        "        bars = ax2.bar(range(len(agents)), reputation_scores, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax2.set_xlabel('Agent ID')\n",
        "        ax2.set_ylabel('Reputation Score')\n",
        "        ax2.set_title('Individual Agent Reputation Scores')\n",
        "        ax2.set_xticks(range(len(agents)))\n",
        "        ax2.set_xticklabels([f'Agent {i}' for i in agents])\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, score in zip(bars, reputation_scores):\n",
        "            height = bar.get_height()\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Initialize the visualizer\n",
        "visualizer = EBSLVisualizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Jupyter-Compatible Async Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_with_loop(func, /, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Jupyter-compatible async execution.\n",
        "    Handles both sync and async functions, with fallbacks for different environments.\n",
        "    \"\"\"\n",
        "    # First try direct execution (for sync functions)\n",
        "    result = func(*args, **kwargs)\n",
        "    \n",
        "    # If it's awaitable, we need special handling\n",
        "    if inspect.isawaitable(result):\n",
        "        try:\n",
        "            # Try nest_asyncio for Jupyter compatibility\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            return asyncio.run(result)\n",
        "        except ImportError:\n",
        "            # Fallback: thread-based execution\n",
        "            import concurrent.futures\n",
        "            \n",
        "            def run_async():\n",
        "                loop = asyncio.new_event_loop()\n",
        "                asyncio.set_event_loop(loop)\n",
        "                try:\n",
        "                    return loop.run_until_complete(result)\n",
        "                finally:\n",
        "                    loop.close()\n",
        "            \n",
        "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "                return executor.submit(run_async).result()\n",
        "        except Exception as e:\n",
        "            # Last resort: try to get the event loop and run\n",
        "            try:\n",
        "                loop = asyncio.get_event_loop()\n",
        "                if loop.is_running():\n",
        "                    # Create a task and let it run\n",
        "                    import concurrent.futures\n",
        "                    \n",
        "                    def run_in_thread():\n",
        "                        new_loop = asyncio.new_event_loop()\n",
        "                        asyncio.set_event_loop(new_loop)\n",
        "                        try:\n",
        "                            return new_loop.run_until_complete(result)\n",
        "                        finally:\n",
        "                            new_loop.close()\n",
        "                    \n",
        "                    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "                        return executor.submit(run_in_thread).result()\n",
        "                else:\n",
        "                    return loop.run_until_complete(result)\n",
        "            except:\n",
        "                # Absolute fallback - might fail in some cases\n",
        "                return asyncio.run(result)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def get_srs_with_fallback(settings_path: str, srs_path: str, logger: Logger) -> bool:\n",
        "    \"\"\"\n",
        "    Try Python binding; if that fails, fall back to `ezkl get-srs -S settings.json`.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ok = run_with_loop(ezkl.get_srs, srs_path=srs_path, settings_path=settings_path)\n",
        "        if ok:\n",
        "            return True\n",
        "        logger.warn(\"ezkl.get_srs returned False; attempting CLI fallback\")\n",
        "    except Exception as e:\n",
        "        logger.warn(f\"ezkl.get_srs raised {e!r}; attempting CLI fallback\")\n",
        "\n",
        "    try:\n",
        "        cmd = [\"ezkl\", \"get-srs\", \"-S\", settings_path]\n",
        "        subprocess.run(cmd, check=True)\n",
        "        if _Path(srs_path).exists():\n",
        "            return True\n",
        "        default_srs = _Path.home() / \".ezkl\" / \"srs\" / \"kzg15.srs\"\n",
        "        if default_srs.exists():\n",
        "            import shutil\n",
        "            shutil.copy2(default_srs, srs_path)\n",
        "            logger.info(f\"Copied SRS from default location to {srs_path}\")\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e2:\n",
        "        logger.error(f\"CLI get-srs failed: {e2!r}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EZKL Safe Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_setattr(obj, name, value, logger: Logger):\n",
        "    \"\"\"Set attribute if it exists in bindings; otherwise log and continue.\"\"\"\n",
        "    try:\n",
        "        setattr(obj, name, value)\n",
        "        logger.info(f\"run_args.{name} = {value!r}\")\n",
        "    except Exception as e:\n",
        "        logger.warn(f\"run_args field {name!r} not supported by this ezkl version: {e!r}\")\n",
        "\n",
        "def safe_calibrate(logger: Logger, *, data, model, settings, **kwargs) -> bool:\n",
        "    \"\"\"\n",
        "    Call ezkl.calibrate_settings with rich kwargs; on TypeError,\n",
        "    progressively back off to a minimal call.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return bool(run_with_loop(ezkl.calibrate_settings, data=data, model=model, settings=settings, **kwargs))\n",
        "    except TypeError as e:\n",
        "        logger.warn(f\"Calibration kwargs not fully supported ({e}); retrying with reduced kwargs\")\n",
        "        # Fallback 1: keep impactful knobs\n",
        "        try_kwargs = {k: kwargs[k] for k in [\"target\", \"lookup_safety_margin\", \"max_logrows\"] if k in kwargs}\n",
        "        try:\n",
        "            return bool(run_with_loop(ezkl.calibrate_settings, data=data, model=model, settings=settings, **try_kwargs))\n",
        "        except TypeError as e2:\n",
        "            logger.warn(f\"Reduced calibration call failed ({e2}); retrying minimal\")\n",
        "            # Fallback 2: minimal signature\n",
        "            return bool(run_with_loop(ezkl.calibrate_settings, data=data, model=model, settings=settings))\n",
        "\n",
        "def summarize_settings(path: str) -> dict:\n",
        "    try:\n",
        "        with open(path, \"r\") as f:\n",
        "            s = json.load(f)\n",
        "        out = {\n",
        "            \"logrows\": s.get(\"logrows\"),\n",
        "            \"input_visibility\": s.get(\"input_visibility\"),\n",
        "            \"param_visibility\": s.get(\"param_visibility\"),\n",
        "            \"output_visibility\": s.get(\"output_visibility\"),\n",
        "        }\n",
        "        if all(v is None for v in out.values()) and isinstance(s, dict):\n",
        "            run_args = s.get(\"run_args\") or s.get(\"py_run_args\") or {}\n",
        "            out.update({\n",
        "                \"input_visibility\": run_args.get(\"input_visibility\", out[\"input_visibility\"]),\n",
        "                \"param_visibility\": run_args.get(\"param_visibility\", out[\"param_visibility\"]),\n",
        "                \"output_visibility\": run_args.get(\"output_visibility\", out[\"output_visibility\"]),\n",
        "            })\n",
        "        return out\n",
        "    except Exception:\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. EBSL Algorithm Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassicalEBSLAlgorithm:\n",
        "    @staticmethod\n",
        "    def fuse(opinions_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        # opinions_tensor: [N, 4] -> [b,d,u,a]\n",
        "        b, d, u, a = [o.squeeze(-1) for o in opinions_tensor.split(1, dim=-1)]\n",
        "        denominator = torch.sum(u, dim=-1) - (opinions_tensor.shape[0] - 1)\n",
        "        if torch.any(denominator == 0):\n",
        "            denominator = denominator + (denominator == 0) * 1e-9\n",
        "        b_fused = torch.sum(b * u, dim=-1) / denominator\n",
        "        d_fused = torch.sum(d * u, dim=-1) / denominator\n",
        "        u_fused = torch.prod(u, dim=-1) / denominator\n",
        "        a_fused = torch.sum((a * u), dim=-1) / denominator\n",
        "        return torch.stack([b_fused, d_fused, u_fused, a_fused], dim=-1)\n",
        "\n",
        "class EBSLAlgorithm:\n",
        "    @staticmethod\n",
        "    def fuse(opinions_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        b, d, u, a = [o.squeeze(-1) for o in opinions_tensor.split(1, dim=-1)]\n",
        "        denominator = torch.sum(u, dim=-1) - (opinions_tensor.shape[0] - 1)\n",
        "        is_zero = (denominator == 0).to(torch.float32)\n",
        "        denominator = denominator + (is_zero * 1e-9)\n",
        "        inv_denominator = torch.reciprocal(denominator)\n",
        "        b_fused = torch.sum(b * u, dim=-1) * inv_denominator\n",
        "        d_fused = torch.sum(d * u, dim=-1) * inv_denominator\n",
        "        u_fused = torch.prod(u, dim=-1) * inv_denominator\n",
        "        a_fused = torch.sum((a * u), dim=-1) * inv_denominator\n",
        "        return torch.stack([b_fused, d_fused, u_fused, a_fused], dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_reputation(final_opinion_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        b, d, u, a = [o.squeeze(-1) for o in final_opinion_tensor.split(1, dim=-1)]\n",
        "        return b + a * u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Property-Based Testing and Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@st.composite\n",
        "def opinion_strategy(draw):\n",
        "    b = draw(st.floats(0.0, 1.0))\n",
        "    d = draw(st.floats(0.0, 1.0 - b))\n",
        "    u = 1.0 - b - d\n",
        "    a = draw(st.floats(0.0, 1.0))\n",
        "    return torch.tensor([b, d, u, a], dtype=torch.float32)\n",
        "\n",
        "@st.composite\n",
        "def opinions_tensor_strategy(draw, min_opinions=2, max_opinions=50):\n",
        "    num_opinions = draw(st.integers(min_opinions, max_opinions))\n",
        "    opinions = draw(st.lists(opinion_strategy(), min_size=num_opinions, max_size=num_opinions))\n",
        "    return torch.stack(opinions)\n",
        "\n",
        "def run_property_based_correctness_test(logger: Logger):\n",
        "    logger.banner(\"Property-based correctness test\")\n",
        "    @given(opinions_tensor=opinions_tensor_strategy())\n",
        "    @hyp_settings(max_examples=100, deadline=None)\n",
        "    def test_fusion_equivalence(opinions_tensor):\n",
        "        classical_result = ClassicalEBSLAlgorithm.fuse(opinions_tensor)\n",
        "        zk_friendly_result = EBSLAlgorithm.fuse(opinions_tensor)\n",
        "        assert torch.allclose(classical_result, zk_friendly_result, atol=1e-6)\n",
        "    with logger.timed(\"hypothesis_equivalence_test\"):\n",
        "        test_fusion_equivalence()\n",
        "    logger.ok(\"Equivalence holds for 100 random examples\")\n",
        "\n",
        "def run_comparative_performance_analysis(logger: Logger, skip_plots: bool = False):\n",
        "    logger.banner(\"Comparative performance analysis\")\n",
        "    opinion_counts = list(range(10, 201, 10))\n",
        "    results = {'classical': [], 'zk_friendly': []}\n",
        "    \n",
        "    # Generate sample data for visualizations\n",
        "    sample_opinions_tensor = None\n",
        "    \n",
        "    with logger.timed(\"perf_benchmark\"):\n",
        "        for count in opinion_counts:\n",
        "            b = torch.rand(count)\n",
        "            d = torch.rand(count) * (1 - b)\n",
        "            u = 1 - b - d\n",
        "            a = torch.rand(count)\n",
        "            sample_tensor = torch.stack([b, d, u, a], dim=1)\n",
        "            \n",
        "            # Save a sample for visualization\n",
        "            if count == 50:  # Use medium-sized sample for visualization\n",
        "                sample_opinions_tensor = sample_tensor.clone()\n",
        "            \n",
        "            t0 = time.perf_counter()\n",
        "            ClassicalEBSLAlgorithm.fuse(sample_tensor)\n",
        "            results['classical'].append(time.perf_counter() - t0)\n",
        "            t0 = time.perf_counter()\n",
        "            EBSLAlgorithm.fuse(sample_tensor)\n",
        "            results['zk_friendly'].append(time.perf_counter() - t0)\n",
        "    \n",
        "    logger.ok(\"Perf benchmark complete\")\n",
        "    \n",
        "    if not skip_plots:\n",
        "        os.makedirs(\"zkml_artifacts\", exist_ok=True)\n",
        "        \n",
        "        # Original performance plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(opinion_counts, results['classical'], marker='o', label='Classical')\n",
        "        plt.plot(opinion_counts, results['zk_friendly'], marker='x', label='ZK-Friendly')\n",
        "        plt.xlabel(\"Number of Opinions to Fuse\")\n",
        "        plt.ylabel(\"Execution Time [s] (log)\")\n",
        "        plt.title(\"Performance Comparison: Classical vs ZK-Friendly EBSL Fusion\")\n",
        "        plt.legend(); plt.grid(True); plt.yscale('log'); plt.tight_layout()\n",
        "        plot_path = os.path.join(\"zkml_artifacts\", \"perf_plot.png\")\n",
        "        plt.savefig(plot_path, dpi=160)\n",
        "        plt.show()\n",
        "        logger.ok(f\"Saved performance plot: {plot_path}\")\n",
        "        \n",
        "        # Enhanced performance visualization using our visualizer\n",
        "        visualizer.plot_performance_comparison(\n",
        "            results['classical'], \n",
        "            results['zk_friendly'], \n",
        "            opinion_counts,\n",
        "            \"Enhanced Performance Analysis: Classical vs ZK-Friendly EBSL\"\n",
        "        )\n",
        "        \n",
        "        # Visualize opinion components from sample data\n",
        "        if sample_opinions_tensor is not None:\n",
        "            logger.info(\"Generating opinion component visualizations...\")\n",
        "            visualizer.visualize_opinion_components(\n",
        "                sample_opinions_tensor,\n",
        "                \"Sample Opinion Components Analysis (50 agents)\"\n",
        "            )\n",
        "            \n",
        "            # Show fusion process with a subset of opinions\n",
        "            subset_opinions = sample_opinions_tensor[:5]  # Use first 5 for clarity\n",
        "            fused_opinion = EBSLAlgorithm.fuse(subset_opinions)\n",
        "            visualizer.plot_fusion_process(\n",
        "                subset_opinions,\n",
        "                fused_opinion,\n",
        "                \"EBSL Fusion Process Demonstration (5 agents)\"\n",
        "            )\n",
        "    \n",
        "    return results, sample_opinions_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ZK-Optimized EBSL Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EBslFusionModule(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    ZK-optimized EBSL fusion module with overflow-safe ops.\n",
        "    Input: combined tensor with opinions and mask flattened and concatenated\n",
        "    Outputs:\n",
        "      fused:    (B, 4)\n",
        "      rep:      (B, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, max_opinions: int = 16):\n",
        "        super().__init__()\n",
        "        self.max_opinions = max_opinions\n",
        "        self.opinions_size = max_opinions * 4  # N * 4 for [b,d,u,a]\n",
        "        self.mask_size = max_opinions         # N for mask\n",
        "        self.register_buffer('epsilon', torch.tensor(1e-6))\n",
        "        self.register_buffer('one', torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, combined_input: torch.Tensor):\n",
        "        batch_size = combined_input.shape[0]\n",
        "\n",
        "        # Split & reshape back to opinions and mask\n",
        "        opinions_flat = combined_input[:, :self.opinions_size]\n",
        "        mask_flat = combined_input[:, self.opinions_size:self.opinions_size + self.mask_size]\n",
        "        opinions = opinions_flat.view(batch_size, self.max_opinions, 4)\n",
        "        mask = mask_flat.view(batch_size, self.max_opinions)\n",
        "\n",
        "        b = opinions[..., 0]\n",
        "        d = opinions[..., 1]\n",
        "        u = opinions[..., 2]\n",
        "        a = opinions[..., 3]\n",
        "\n",
        "        m = mask\n",
        "        K = torch.sum(m, dim=1)\n",
        "\n",
        "        sum_bu = torch.sum((b * u) * m, dim=1)\n",
        "        sum_du = torch.sum((d * u) * m, dim=1)\n",
        "        sum_au = torch.sum((a * u) * m, dim=1)\n",
        "        sum_u  = torch.sum(u * m, dim=1)\n",
        "\n",
        "        # Stable product via logs (avoids huge intermediates)\n",
        "        u_masked = u * m + (self.one - m)                # 1 for masked-out entries\n",
        "        u_clamped = torch.clamp(u_masked, min=self.epsilon, max=self.one)\n",
        "        sum_log = torch.sum(torch.log(u_clamped), dim=1)\n",
        "        prod_u = torch.exp(sum_log)\n",
        "\n",
        "        # Sign-preserving denom clamp\n",
        "        denom = sum_u - K + self.one\n",
        "        denom_sign = torch.where(denom >= 0, self.one, -self.one)\n",
        "        denom = denom_sign * torch.clamp(torch.abs(denom), min=self.epsilon)\n",
        "\n",
        "        b_f = sum_bu / denom\n",
        "        d_f = sum_du / denom\n",
        "        u_f = prod_u / denom\n",
        "        a_f = sum_au / denom\n",
        "\n",
        "        fused = torch.stack([b_f, d_f, u_f, a_f], dim=1)\n",
        "        rep = (b_f + a_f * u_f).unsqueeze(1)\n",
        "        return fused, rep\n",
        "\n",
        "def _gen_synthetic_opinions(N: int):\n",
        "    b = torch.rand(N)\n",
        "    d = torch.rand(N) * (1.0 - b)\n",
        "    u = 1.0 - b - d\n",
        "    a = torch.rand(N)\n",
        "    opinions = torch.stack([b, d, u, a], dim=1)\n",
        "    mask = torch.ones(N)\n",
        "    return opinions, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ZKML Pipeline Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_zkml_pipeline_with_ebsl(logger: Logger, max_opinions: int = 16,\n",
        "                               zk_strategy: str = \"balanced\",\n",
        "                               manual_input_scale: int = None,\n",
        "                               manual_param_scale: int = None,\n",
        "                               skip_calibration: bool = False):\n",
        "    logger.banner(\"ZKML pipeline: EBSL fusion in EZKL\")\n",
        "    wd = os.path.abspath(\"zkml_artifacts\")\n",
        "    os.makedirs(wd, exist_ok=True)\n",
        "\n",
        "    # 1) Export ONNX\n",
        "    with logger.timed(\"export_onnx\", extra={\"max_opinions\": max_opinions}) as info:\n",
        "        model = EBslFusionModule(max_opinions=max_opinions).eval()\n",
        "        opinions, mask = _gen_synthetic_opinions(max_opinions)\n",
        "        opinions_b, mask_b = opinions.unsqueeze(0), mask.unsqueeze(0)\n",
        "\n",
        "        # Create combined input for single-input model\n",
        "        opinions_flat = opinions_b.flatten(start_dim=1)\n",
        "        mask_flat = mask_b.flatten(start_dim=1)\n",
        "        combined_input = torch.cat([opinions_flat, mask_flat], dim=1)\n",
        "\n",
        "        onnx_path = os.path.join(wd, \"ebsl_model.onnx\")\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            combined_input,\n",
        "            onnx_path,\n",
        "            input_names=[\"combined_input\"],\n",
        "            output_names=[\"fused\", \"rep\"],\n",
        "            opset_version=13,\n",
        "            dynamic_axes=None,\n",
        "        )\n",
        "        info[\"onnx_path\"] = onnx_path\n",
        "        logger.ok(f\"Exported ONNX -> {onnx_path}\")\n",
        "        if logger.verbose:\n",
        "            logger.info(f\"combined input shape: {tuple(combined_input.shape)}\")\n",
        "            logger.info(\"sample opinions[0,:3]: \" + json.dumps(opinions_b[0, :3].detach().numpy().tolist(), indent=2))\n",
        "\n",
        "    # 2) gen_settings\n",
        "    with logger.timed(\"gen_settings\") as info:\n",
        "        run_args = ezkl.PyRunArgs()\n",
        "        # visibility\n",
        "        safe_setattr(run_args, \"input_visibility\",  \"public\", logger)\n",
        "        safe_setattr(run_args, \"param_visibility\",  \"fixed\",  logger)\n",
        "        safe_setattr(run_args, \"output_visibility\", \"public\", logger)\n",
        "\n",
        "        # packing & rebasing defenses\n",
        "        safe_setattr(run_args, \"decomp_base\", 16384, logger)          # 2^14 base\n",
        "        safe_setattr(run_args, \"decomp_legs\", 4, logger)              # 4 limbs => ~56-bit capacity\n",
        "        safe_setattr(run_args, \"div_rebasing\", True, logger)          # may be absent; safe_setattr handles it\n",
        "        safe_setattr(run_args, \"scale_rebase_multiplier\", 2, logger)\n",
        "        safe_setattr(run_args, \"check_mode\", \"safe\", logger)\n",
        "\n",
        "        # scales\n",
        "        if manual_input_scale is not None:\n",
        "            safe_setattr(run_args, \"input_scale\", manual_input_scale, logger)\n",
        "            safe_setattr(run_args, \"param_scale\", manual_param_scale or manual_input_scale, logger)\n",
        "            logger.info(f\"Using manual scales: input={manual_input_scale}, param={manual_param_scale or manual_input_scale}\")\n",
        "        elif zk_strategy == \"conservative\":\n",
        "            safe_setattr(run_args, \"input_scale\", 4, logger)\n",
        "            safe_setattr(run_args, \"param_scale\", 4, logger)\n",
        "            logger.info(\"Using conservative ZK strategy (scale=4)\")\n",
        "        elif zk_strategy == \"aggressive\":\n",
        "            safe_setattr(run_args, \"input_scale\", 8, logger)\n",
        "            safe_setattr(run_args, \"param_scale\", 8, logger)\n",
        "            logger.info(\"Using aggressive ZK strategy (scale=8)\")\n",
        "        else:  # balanced\n",
        "            safe_setattr(run_args, \"input_scale\", 6, logger)\n",
        "            safe_setattr(run_args, \"param_scale\", 6, logger)\n",
        "            logger.info(\"Using balanced ZK strategy (scale=6)\")\n",
        "\n",
        "        settings_path = os.path.join(wd, \"settings.json\")\n",
        "        # BUGFIX: use the outer-scope onnx_path, not info[\"onnx_path\"]\n",
        "        ok = run_with_loop(ezkl.gen_settings, model=onnx_path, output=settings_path, py_run_args=run_args)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"gen_settings failed\")\n",
        "        info[\"settings_path\"] = settings_path\n",
        "        info[\"summary\"] = summarize_settings(settings_path)\n",
        "        logger.ok(f\"Generated settings -> {settings_path}\")\n",
        "        logger.info(\"settings summary: \" + json.dumps(info[\"summary\"], indent=2))\n",
        "\n",
        "    return onnx_path, settings_path  # Return for continuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Complete Pipeline Continuation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def continue_zkml_pipeline(logger: Logger, onnx_path: str, settings_path: str, \n",
        "                          max_opinions: int = 16, skip_calibration: bool = False):\n",
        "    \"\"\"\n",
        "    Continue the ZKML pipeline from where run_zkml_pipeline_with_ebsl left off.\n",
        "    \"\"\"\n",
        "    wd = os.path.dirname(settings_path)\n",
        "    \n",
        "    # 3) input.json (GraphData: single input vector)\n",
        "    with logger.timed(\"write_input_json\") as info:\n",
        "        opinions, mask = _gen_synthetic_opinions(max_opinions)\n",
        "        opinions_b, mask_b = opinions.unsqueeze(0), mask.unsqueeze(0)\n",
        "        opinions_data = opinions_b.detach().cpu().numpy().flatten().tolist()\n",
        "        mask_data = mask_b.detach().cpu().numpy().flatten().tolist()\n",
        "        combined_input_list = opinions_data + mask_data\n",
        "\n",
        "        graph_data = {\n",
        "            \"input_data\": [combined_input_list],\n",
        "            \"input_shapes\": [[len(combined_input_list)]]\n",
        "        }\n",
        "\n",
        "        input_json = os.path.join(wd, \"input.json\")\n",
        "        with open(input_json, \"w\") as f:\n",
        "            json.dump(graph_data, f)\n",
        "        info[\"input_json\"] = input_json\n",
        "        logger.ok(f\"Wrote input -> {input_json}\")\n",
        "\n",
        "    # 4) calibrate_settings (robust)\n",
        "    with logger.timed(\"calibrate_settings\") as info:\n",
        "        if skip_calibration:\n",
        "            logger.info(\"Skipping calibration (skip_calibration=True)\")\n",
        "            info[\"calibrated\"] = False\n",
        "            info[\"skipped\"] = True\n",
        "        else:\n",
        "            cal_kwargs = dict(\n",
        "                target=\"resources\",\n",
        "                lookup_safety_margin=2,\n",
        "                scales=[6, 8, 10, 12],\n",
        "                scale_rebase_multiplier=[1, 2, 4],\n",
        "                max_logrows=16,\n",
        "            )\n",
        "            try:\n",
        "                cal_ok = safe_calibrate(logger, data=input_json, model=onnx_path, settings=settings_path, **cal_kwargs)\n",
        "                info[\"calibrated\"] = bool(cal_ok)\n",
        "                if cal_ok:\n",
        "                    logger.ok(\"Settings calibrated successfully\")\n",
        "                else:\n",
        "                    logger.warn(\"Calibration returned False - using fallback settings\")\n",
        "            except Exception as e:\n",
        "                info[\"calibration_exception\"] = repr(e)\n",
        "                logger.warn(f\"Calibration failed: {e}\")\n",
        "\n",
        "    # 5) compile_circuit\n",
        "    with logger.timed(\"compile_circuit\") as info:\n",
        "        compiled_path = os.path.join(wd, \"compiled.onnx\")\n",
        "        ok = run_with_loop(ezkl.compile_circuit, model=onnx_path, compiled_circuit=compiled_path, settings_path=settings_path)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"compile_circuit failed\")\n",
        "        info[\"compiled_path\"] = compiled_path\n",
        "        logger.ok(f\"Compiled circuit -> {compiled_path}\")\n",
        "        try:\n",
        "            circuit_size = os.path.getsize(compiled_path)\n",
        "            info[\"circuit_size_bytes\"] = circuit_size\n",
        "            logger.info(f\"Compiled circuit size: {circuit_size:,} bytes ({circuit_size/1024:.1f} KB)\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 6) get_srs\n",
        "    with logger.timed(\"get_srs\") as info:\n",
        "        srs_path = os.path.join(wd, \"kzg.srs\")\n",
        "        ok = get_srs_with_fallback(settings_path=settings_path, srs_path=srs_path, logger=logger)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"get_srs failed\")\n",
        "        info[\"srs_path\"] = srs_path\n",
        "        logger.ok(f\"SRS ready -> {srs_path}\")\n",
        "\n",
        "    return compiled_path, srs_path, input_json\n",
        "\n",
        "def complete_proof_generation(logger: Logger, compiled_path: str, srs_path: str, \n",
        "                             input_json: str, settings_path: str, max_opinions: int = 16):\n",
        "    \"\"\"\n",
        "    Complete the proof generation phase.\n",
        "    \"\"\"\n",
        "    wd = os.path.dirname(compiled_path)\n",
        "    \n",
        "    # 7) setup\n",
        "    with logger.timed(\"setup\") as info:\n",
        "        pk_path = os.path.join(wd, \"model.pk\")\n",
        "        vk_path = os.path.join(wd, \"model.vk\")\n",
        "        ok = run_with_loop(ezkl.setup, model=compiled_path, vk_path=vk_path, pk_path=pk_path, srs_path=srs_path)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"setup failed\")\n",
        "        info[\"pk_path\"] = pk_path\n",
        "        info[\"vk_path\"] = vk_path\n",
        "        logger.ok(f\"Setup complete -> pk:{pk_path}, vk:{vk_path}\")\n",
        "\n",
        "    # 8) gen_witness\n",
        "    with logger.timed(\"gen_witness\") as info:\n",
        "        witness_path = os.path.join(wd, \"witness.json\")\n",
        "        ok = run_with_loop(ezkl.gen_witness, data=input_json, model=compiled_path, output=witness_path)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"gen_witness failed\")\n",
        "        info[\"witness_path\"] = witness_path\n",
        "        \n",
        "        # Torch peek for numerical accuracy comparison\n",
        "        with torch.no_grad():\n",
        "            opinions, mask = _gen_synthetic_opinions(max_opinions)\n",
        "            opinions_b, mask_b = opinions.unsqueeze(0), mask.unsqueeze(0)\n",
        "            opinions_flat = opinions_b.flatten(start_dim=1)\n",
        "            mask_flat = mask_b.flatten(start_dim=1)\n",
        "            combined_input = torch.cat([opinions_flat, mask_flat], dim=1)\n",
        "            fused_t, rep_t = EBslFusionModule(max_opinions=max_opinions).eval()(combined_input)\n",
        "        info[\"torch_fused\"] = fused_t[0].detach().cpu().numpy().tolist()\n",
        "        info[\"torch_rep\"] = rep_t[0].detach().cpu().numpy().tolist()\n",
        "        logger.ok(f\"Witness generated -> {witness_path}\")\n",
        "        logger.info(\"Torch fused: \" + json.dumps(info[\"torch_fused\"], indent=2))\n",
        "        logger.info(\"Torch rep:   \" + json.dumps(info[\"torch_rep\"], indent=2))\n",
        "\n",
        "    # 9) mock\n",
        "    with logger.timed(\"mock\"):\n",
        "        ok = run_with_loop(ezkl.mock, witness=witness_path, model=compiled_path)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"mock failed\")\n",
        "        logger.ok(\"Mock successful\")\n",
        "\n",
        "    # 10) prove\n",
        "    with logger.timed(\"prove\") as info:\n",
        "        proof_path = os.path.join(wd, \"proof.pf\")\n",
        "        ok = run_with_loop(\n",
        "            ezkl.prove,\n",
        "            witness=witness_path, model=compiled_path, pk_path=pk_path,\n",
        "            proof_path=proof_path, srs_path=srs_path, proof_type=\"single\"\n",
        "        )\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"prove failed\")\n",
        "        info[\"proof_path\"] = proof_path\n",
        "        try:\n",
        "            proof_size = os.path.getsize(proof_path)\n",
        "            info[\"proof_size_bytes\"] = proof_size\n",
        "            logger.ok(f\"Proof generated -> {proof_path} ({proof_size:,} bytes, {proof_size/1024:.1f} KB)\")\n",
        "        except Exception:\n",
        "            logger.ok(f\"Proof generated -> {proof_path}\")\n",
        "\n",
        "    # 11) verify\n",
        "    with logger.timed(\"verify\") as info:\n",
        "        ok = run_with_loop(ezkl.verify, proof_path=proof_path, settings_path=settings_path, vk_path=vk_path, srs_path=srs_path)\n",
        "        info[\"verified\"] = bool(ok)\n",
        "        if ok:\n",
        "            logger.ok(\"Proof verified ✅\")\n",
        "        else:\n",
        "            logger.error(\"Proof verification failed ❌\")\n",
        "    \n",
        "    return proof_path, info[\"verified\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Demo Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize logger\n",
        "logger = Logger(verbose=True)\n",
        "\n",
        "# Configuration\n",
        "max_opinions = 8  # Smaller for demo\n",
        "zk_strategy = \"balanced\"\n",
        "skip_calibration = False  # Calibration required for proper functioning\n",
        "\n",
        "logger.banner(\"EBSL + EZKL Demo Pipeline\")\n",
        "print(f\"Configuration: max_opinions={max_opinions}, strategy={zk_strategy}, skip_calibration={skip_calibration}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Topology and ZK Circuit Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the trust network topology\n",
        "logger.info(\"Generating network topology visualization...\")\n",
        "trust_matrix = visualizer.create_network_topology(\n",
        "    num_nodes=max_opinions,\n",
        "    title=f\"Trust Network Topology ({max_opinions} agents)\"\n",
        ")\n",
        "\n",
        "# Show the ZK circuit concept\n",
        "logger.info(\"Generating ZK circuit concept visualization...\")\n",
        "visualizer.visualize_zk_circuit_concept(\n",
        "    \"EBSL Zero-Knowledge Circuit Architecture\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Property-based correctness test\n",
        "try:\n",
        "    run_property_based_correctness_test(logger)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Property test failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Performance analysis\n",
        "try:\n",
        "    run_comparative_performance_analysis(logger, skip_plots=False)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Performance analysis failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: ZKML Pipeline - Initial Setup\n",
        "try:\n",
        "    onnx_path, settings_path = run_zkml_pipeline_with_ebsl(\n",
        "        logger, \n",
        "        max_opinions=max_opinions,\n",
        "        zk_strategy=zk_strategy,\n",
        "        skip_calibration=skip_calibration\n",
        "    )\n",
        "    logger.ok(\"Initial pipeline setup completed\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Initial setup failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Continue Pipeline - Compilation\n",
        "try:\n",
        "    compiled_path, srs_path, input_json = continue_zkml_pipeline(\n",
        "        logger, onnx_path, settings_path, max_opinions, skip_calibration\n",
        "    )\n",
        "    logger.ok(\"Pipeline compilation completed\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Pipeline compilation failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Complete Proof Generation\n",
        "try:\n",
        "    proof_path, verified = complete_proof_generation(\n",
        "        logger, compiled_path, srs_path, input_json, settings_path, max_opinions\n",
        "    )\n",
        "    if verified:\n",
        "        logger.ok(\"🎉 Complete EBSL+EZKL pipeline successful!\")\n",
        "    else:\n",
        "        logger.error(\"❌ Proof verification failed\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Proof generation failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reputation Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and visualize reputation scores\n",
        "try:\n",
        "    logger.info(\"Generating reputation analysis...\")\n",
        "    \n",
        "    # Create sample opinions for reputation calculation\n",
        "    np.random.seed(42)  # For reproducible results\n",
        "    num_agents = max_opinions\n",
        "    \n",
        "    # Generate diverse opinion profiles\n",
        "    beliefs = np.random.beta(2, 3, num_agents)  # Skewed towards lower belief\n",
        "    disbeliefs = np.random.beta(1.5, 4, num_agents) * (1 - beliefs)\n",
        "    uncertainties = 1 - beliefs - disbeliefs\n",
        "    base_rates = np.random.uniform(0.3, 0.7, num_agents)  # Reasonable base rates\n",
        "    \n",
        "    opinions_tensor = torch.tensor(np.column_stack([beliefs, disbeliefs, uncertainties, base_rates]), dtype=torch.float32)\n",
        "    \n",
        "    # Calculate reputation scores\n",
        "    reputation_scores = []\n",
        "    for i in range(num_agents):\n",
        "        individual_opinion = opinions_tensor[i:i+1]  # Single opinion\n",
        "        reputation = EBSLAlgorithm.calculate_reputation(individual_opinion).item()\n",
        "        reputation_scores.append(reputation)\n",
        "    \n",
        "    # Visualize reputation scores\n",
        "    visualizer.plot_reputation_evolution(\n",
        "        reputation_scores,\n",
        "        list(range(num_agents)),\n",
        "        f\"Agent Reputation Scores (n={num_agents})\"\n",
        "    )\n",
        "    \n",
        "    # Create a comprehensive trust network with calculated reputations\n",
        "    logger.info(\"Creating comprehensive trust analysis...\")\n",
        "    \n",
        "    # Use reputation scores to create weighted trust matrix\n",
        "    trust_matrix_weighted = np.zeros((num_agents, num_agents))\n",
        "    for i in range(num_agents):\n",
        "        for j in range(num_agents):\n",
        "            if i != j:\n",
        "                # Trust is influenced by target's reputation and relationship strength\n",
        "                base_trust = trust_matrix[i, j] if 'trust_matrix' in locals() else np.random.beta(2, 5)\n",
        "                reputation_factor = reputation_scores[j]\n",
        "                trust_matrix_weighted[i, j] = base_trust * (0.5 + 0.5 * reputation_factor)\n",
        "            else:\n",
        "                trust_matrix_weighted[i, j] = 1.0  # Self-trust\n",
        "    \n",
        "    # Visualize the reputation-weighted network\n",
        "    visualizer.create_network_topology(\n",
        "        num_nodes=num_agents,\n",
        "        trust_matrix=trust_matrix_weighted,\n",
        "        title=f\"Reputation-Weighted Trust Network ({num_agents} agents)\"\n",
        "    )\n",
        "    \n",
        "    logger.ok(f\"Reputation analysis complete. Average reputation: {np.mean(reputation_scores):.3f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Reputation analysis failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Generate Report\n",
        "try:\n",
        "    report_path = os.path.join(\"zkml_artifacts\", \"run_report.json\")\n",
        "    logger.dump_report(report_path)\n",
        "    \n",
        "    # Summary\n",
        "    total_time = sum(step.seconds for step in logger.steps)\n",
        "    success_count = sum(1 for step in logger.steps if step.ok)\n",
        "    total_steps = len(logger.steps)\n",
        "    \n",
        "    logger.banner(\"Pipeline Summary\")\n",
        "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
        "    print(f\"Successful steps: {success_count}/{total_steps}\")\n",
        "    print(f\"Success rate: {success_count/total_steps*100:.1f}%\")\n",
        "    \n",
        "    if verified:\n",
        "        print(\"\\n✅ EBSL+EZKL pipeline completed successfully!\")\n",
        "        print(\"🔒 Zero-knowledge proof generated and verified\")\n",
        "        print(\"📊 EBSL fusion algorithm working in ZK environment\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  Pipeline completed with verification issues\")\n",
        "        \n",
        "except Exception as e:\n",
        "    logger.error(f\"Report generation failed: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}